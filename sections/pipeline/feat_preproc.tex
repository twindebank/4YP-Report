\section{Pre-Processing Features}
\label{sec:pl-featpreproc}
    Before using the features for classification input, there are a number of techniques that can be employed to increase both the speed and quality of predictions. Discussion and evaluation, in regards to mosquito detection, of the methods detailed in this section, are left to section \ref{sec:exp-clf}.

    \subsection{Normalisation}
    \label{subsec:pl-featpreproc-norm}
        Normalisation, also known as feature (re)scaling or standardisation, can be carried out in a range of ways. It is important for some classification algorithms that make decisions based on a distance metric in the feature domain; if one feature is on a larger scale to another, then the smaller-scale feature may have less influence in classification decisions, depending on the classifier.     
        The simplest and most common method is to apply an affine transform which `centres' and `whitens' each column of the feature matrix, meaning the mean is transformed to zero and the variance is scaled to unit variance.
        
        This process comes with a caveat; if the whole feature matrix is normalised, then split into training and testing sets, then information from the test set `leaks' into the training set. Applying the scaling individually to the training and test set is also incorrect as the transformations applied to each matrix will be different, depending on the data, leading to high sensitivity of outliers. Instead, the training feature matrix should be normalised, then the means and variances reused for the test set, meaning the same transformation is applied and scaling is correct.

    \subsection{Feature Selection}
    \label{subsec:pl-featpreproc-sel}
        The quality and reliability of predictions using traditional classifiers rely heavily on the input features. One intuitive approach is to generate a large set of intrinsic features, such as those listed in table \ref{tbl:pl-feats-audio-feattbl}, and use them all. Problems with this approach include slower training and classification time as well as reduced performance in some cases, compared to using single features in isolation due to uninformative features and information redundancy. The selection of salient features is therefore an important step preceding classification. 
        
        \begin{listing}[ht]
            \begin{minted}[frame=single,framesep=2mm, bgcolor=black!2, fontsize=\footnotesize, linenos]{python}
import numpy as np
def rfe(featmat_trn, featmat_tst, truth_trn, truth_tst, stepsize, classifier):
    n_feats, scores = featmat_trn.size[1], []
    steps = np.arange(1,n_feats,stepsize)[::-1]             # linear decreasing steps
    for n in steps:          
        classifier.fit(featmat_trn,truth_trn)               # fit classifier object
        pred = classifier.predict(featmat_tst)              # generate predictions
        scores.append(calc_score(pred,truth_tst))           # test predictions and store score
        feat_imprt = classifier.feature_importances         # get feature importances
        min_imprt = \                                 # get indices of least important features
            [feat_imprt.index(minval) for minval in reversed(sorted(feat_imprt))[0:stepsize]] 
        featmat_trn = np.delete(featmat_trn,min_imprt,axis=1) 
        featmat_tst = np.delete(featmat_tst,min_imprt,axis=1)
    return steps, scores
            \end{minted}
            \caption{Recursive Feature Elimination}
            \label{code:pl-featpreproc-sel-rfe}
        \end{listing}
        
        Two methods of feature selection are explored in this report; recursive feature elimination, \textit{RFE}; and feature-space truncation using principal component analysis, \textit{PCA}. Recursive feature elimination is a method that can be used with any classifier that reveals how it has used each feature to make a decision, i.e. how important it decides each feature is. Of the six implemented classifiers, two classifier implementations include attributes of this nature: the SVM with a linear kernel \cite{Guyon2002}, and the random forest \cite{Granitto2006}. Each iteration of RFE requires results from the previous iteration, meaning the process can not be parallelised. This limits RFE to the random forest only, as the training and prediction times for the linear SVM are too high to be practical. Random forest RFE is first described by \textcite{Granitto2006} and outlined in listing \ref{code:pl-featpreproc-sel-rfe} in simplified Python. MozzPy implements scikit-learn's function \code{RFE\_CV} \cite{Pedregosa2012}, which includes cross-validation code, this greatly simplifies code at the cost of being limited to optimise in regards to only one scoring metric. The reduced feature matrix that gives the best results is then chosen and the indices for features to be removed are saved.
        
        
        Dimensional reduction using PCA is the second implemented feature selection algorithm. PCA can be seen as a way of transforming the feature matrices by projecting them onto their principal axes \cite{Klema1980}. Given a centred feature matrix $X\in\mathbb{R}^{n\times d}$, the first step is to compute the covariance or correlation (normalised covariance) matrix, given by
        \begin{talign}
            \matr{C} = \matr{X}^\text{T}\matr{X}/(n-1).
        \end{talign}
        % As this is a symmetric matrix, it can be diagonalised using singular value decomposition:
        Performing singular value decomposition on the feature matrix gives
        \begin{talign}
            \matr{X} = \matr{USV}^\text{T},
        \end{talign}
        where $\matr{S}$ is the diagonal matrix of singular values, $\matr{U}$ consists of the $d$ orthonormal eigenvectors (left singular vectors) of $\matr{AA}^\text{T}$ corresponding to the $d$ largest eigenvalues; similarly, $\matr{V}$ consists of the orthonormal eigenvectors (right singular vectors) of $\matr{A}^\text{T}\matr{A}$. By substituting this into the covariance matrix,
        \begin{talign}
            \matr{C} &= \matr{VSU}^\text{T}\matr{USV}^\text{T}/(n-1)\\
              &= \matr{V}\frac{\matr{S}^2}{(n-1)}\matr{V}^\text{T}.
        \end{talign}
        As $\matr{S}^2$ is diagonal, this means the right singular vectors, $\matr{V}$, are principal components of the covariance matrix and the eigenvalues are given by $\lambda_i = s^2_i/(n-1)$. The principal components are then calculated as $\matr{P} = \matr{XV}$.
        % \begin{talign}
        %     \matr{XV} = \matr{USV}^\text{T}\matr{V} = \matr{US}.
        % \end{talign}
        These principal components are a mapping of the original feature-space onto axes in which the data varies the most. Over a range of values, the principal components are then truncated in given step sizes. This algorithm is outlined in listing \ref{code:pl-featpreproc-sel-pca}, where PCA is performed with a scikit-learn implementation \cite{Pedregosa2012} and parallelised as reduction at each step is an independent operation of previous reductions. Again, it is emphasised that the same transformation applied on the training data must be applied on the testing data, so $\matr{V}$ used to transform the training data is also used to transform the test data.
        
        \begin{listing}[ht]
            \begin{minted}[frame=single,framesep=2mm, bgcolor=black!2, fontsize=\footnotesize, linenos]{python}
import numpy as np
from sklearn.decomposition import PCA
def pca_reduce(featmat_trn, featmat_tst, truth_trn, truth_tst, steps, classifier):
    n_feats, scores = featmat_trn.size[1], []
    for n in steps:
        pca = PCA(n_components=n)                   # initialise sklearn PCA object
        pca = pca.fit(featmat_trn)                  # fit it to training data
        # apply transform on training and test data
        x_trn_pca, x_tst_pca = pca.transfrom(featmat_trn), pca.transform(featmat_tst)
        classifier.fit(x_trn_pca,truth_trn)         # fit classifier object on transformed data
        pred = classifier.predict(x_tst_pca)        # generate predictions
        scores.append(calc_score(pred,truth_tst))   # test predictions and store score
    return scores
            \end{minted}
            \caption{Principal Component Dimension Reduction}
            \label{code:pl-featpreproc-sel-pca}
        \end{listing} 
    
    \subsection{Balancing Classes}
    \label{subsec:pl-featpreproc-bal}
        Imbalanced classes, i.e. more samples of `no mosquito' than there is `mosquito', can cause the dominant class to be over-represented in the predictions the classifier makes. To balance the data, samples are removed at random from the over-represented class along with the corresponding labels. As discussed in section \ref{subsubsec:exp-clf-opt-class}, there are other techniques available to deal with imbalanced classes but it is found that this random undersampling is sufficient for this context.
    
    \subsection{Software Implementation}
    \label{subsec:pl-featpreproc-software}
        Feature pre-processers are modular, the specified inputs are: the feature matrix, pre-processing arguments, feature information such as names and lengths, and any previous pre-processing data such as a fitted PCA object or normalisation constants from the training data.
        Once audio files, features and labels are imported and stored into a \code{DataSet} object, a \code{ClassiSet} object is initialised and subsequently trained using a split \code{DataSet}. On calling of the \code{train()} method, selected features are taken from the \code{FeatureSet} and pre-processed according to the input arguments, where the final form passed to the classifier is a NumPy array with rows corresponding to samples and columns corresponding to features. Every random operation including and subsequent to this stage is seeded with the same random seed to ensure results are repeatable.
    