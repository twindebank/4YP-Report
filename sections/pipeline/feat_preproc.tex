\section{Pre-processing Features}
\label{sec:pl-featpreproc}

    \subsection{Normalisation}
    \label{subsec:pl-featpreproc-norm}
        \begin{sitemize}
            \item{importance of normalisation and reusing constants etc}
        \end{sitemize}

    \subsection{Feature Selection}
    \label{subsec:pl-featpreproc-sel}
        \begin{sitemize}
            \item{methods of feature selection}
            \item{some maths}
            The degree of dimensional reduction is determined by iterating these algorithms using a random forest. The random forest classifier many times faster than the SVM and also performs considerabley better than the naive Bayes classifier. Using the feature importance attribute, RFE iterates and removes features of least importance etc...
            
            done own implementation rather than sklearn so can graph output and svm too slow on this dataset
            for svm to be reasonable it has to take steps of 30ish features at a time, rf performance muchbeyyer also other methods wont give much more performance improvement, 
            %Granitto2006 - original RF-RFE
            %Guyon2002 - original SVM-RFE, more traditional
        \end{sitemize}
        
    
    \subsection{Balancing Classes}
    \label{subsec:pl-featpreproc-bal}
        \begin{sitemize}
            \item{why balance classes and effect it has (details with different classifiers)}
        \end{sitemize}
    
    \subsection{Software Implementation}
    \label{subsec:pl-featpreproc-software}
        \begin{sitemize}
            \item{modularity}
            \item{how preprocessed features stored}
        \end{sitemize}
    