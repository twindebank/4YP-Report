\section{Classification}
\label{sec:pl-clf}

    \subsection{Supervised Machine Learning}
    \label{subsec:pl-clf-sup}
        The nature of this problem, inferring labels from new data given labelled training data, presents itself to the vast area of supervised machine learning. Machine learning is a data-driven approach which differs to classical algorithms. Instead of, for example, an algorithm that runs peak detection over a frequency representation of a signal looking for peaks in a particular place, a supervised data-driven approach will learn a function, $f$ that maps inputs, $X_i, i\in\{1,...,d\}$, where \textit{d} is the dimension of the feature-space, to the desired class labels, $y_k,k\in\in\{1,...,C\}$. Supervised machine learning is a subset of machine learning which requires labelled examples; whereas, unsupervised learning draws inferences from data to learn latent structures and relationships within the data.
        
        Nine classification configurations are tested in section X, three are chosen to develop for the mosquito detection pipeline. Supervised classification models can be divided into discriminative and generative models.  The first, and simplest both conceptually and computationally, is the  Gaussian naive Bayes (NB) classifier. The NB classifier is a generative model which explicitly learns the joint distribution of the input data and labels, $P(X1,...,X_d,Y)$. From the joint distribution, the probability of a label given the data, i.e. the posterior $P(Y=y_k|X_1,...,X_d)$, can be calculated using Bayes' rule:
        \begin{talign}
            P(Y=y_k|X_1,...,X_d) = \frac{P(X_1,...,X_n,Y=y_k)}{P(X_1,...,X_d)} = \frac{P(X_1,...,X_d|y_k)P(y_k)}{\sum^{C}_{k=1}P(X_1,...,X_d|y_k)P(Y=y_k)},
            \label{eq:pl-clf-sup-bay}
        \end{talign}
        where $P(X_1,...,X_d)$ is the \textit{evidence} and $P(Y=y_k)$ is the class \textit{prior}. Conversely, $P(X_1,...,X_d|Y=y_k)$ may also be calculated be and used to generate likely training pairs $(X_1,...,X_d,y_k)$, hence the name \textit{generative}. The NB classifier operates by first calculating class priors, $P(Y=y_k)$, for all $k\in\{1,...,C\}$: $P(Y=y_k) = \frac{N_{y_k}}{N}$.
        An expression for the likelihood, $P(X_1,...,X_d|Y=y_k)$, is then required; two assumptions are made. First, the probability of $\{X_1,...,X_d\}$ simplified under the assumption that the $X_i$s are conditionally independent over $Y$,
        \begin{talign}
            P(X_1,...,X_d|Y=y_k) = \prod_iP(X_i|Y=y_k).
        \end{talign}
        This is to say that each $X_i$ can be \textit{independently} estimated from the joint probability. This decouples the likelihood computation, greatly reducing complexity at the expense of making an incorrect assumption, hence the `naive' descriptor. In practise, surprisingly good performance can be achieved \cite{Friedman1997}. As each $X_i$ is continuous, a distribution must be assumed for the likelihood -- this is termed the \textit{event model} of the classifier \cite{Mccallum}. A Gaussian event model is commonly assumed, with generative parameters estimated using a maximum likelihood estimate (MLE); but others are also used such as the multinomial and Bernoulli distributions.
        Utilising said assumptions in equation \ref{eq:pl-clf-sup-bay} gives the final expression for the posterior,
        \begin{talign}
            P(Y=y_k|X_1,...,X_d) = \frac{P(Y=y_k)\prod_iP(X_i|y_k)}{\sum^{C}_{k=1}P(Y=y_k)\prod_iP(X_i|y_k)}.
        \end{talign}
        This probabilistic expression is combined with a maximum a posteriori (MAP) decision rule in which the most probable hypothesis is chosen,
        \begin{talign}
            \hat{y} = \argmax_{y_k}P(Y=y_k|X_1,...,X_d),
        \end{talign}
        giving the classification rule for a Gaussian naive Bayes classifier.
        
        % http://www.ic.unicamp.br/~rocha/teaching/2011s1/mc906/aulas/naive-bayes.pdf
        % http://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture5.pdf
        The second classifier is a random forest (RF) \cite{Breiman2001}. The RF classifier is a discriminative algorithm that only cares to learn the distribution of $Y$ conditioned on the input features $\{X_1,...,X_d\}$, the most likely class is chosen considering the $X_i$s only; in the feature-space this maps to learning a \textit{decision boundary}. The RF is an ensemble method, it combines many weak learners, \textit{decision trees}, into a single strong learner. A decision tree is a greedy approach that `grows' top down, splitting the data by a single attribute at each node in a way that locally minimises entropy, given by
        \begin{talign}
            H[X] = -\sum^n_{j=1}p(x_j)\log_2p(x_j).
        \end{talign}
        At every node, the entropy of the parent node is calculated and a series of potential splits are made. The weighted combinations of the entropy of the child nodes are calculated for each split and the split that maximises information gain,
        \begin{talign}
            I[X] = H[X]_{\text{parent}} - \left(\frac{N_{\text{childA}}}{N_{\text{parent}}}\times H[X_{\text{childA}}] - \frac{N_{\text{childB}}}{N_{\text{parent}}}\times H[X_{\text{childB}}]\right),
        \end{talign}
        is chosen. The process is repeated recursively on nodes down the tree until a node contains data from only a single class -- a terminal node. Decision trees are prone to overfitting data; random forests, however, are not. RFs are a collection of unbound decision trees that accept a random subset of data, usually around 66\% with replacement \cite{Breiman2001}. For classification, each tree then `votes' and the majority vote is accepted as the output of the random forest. In addition to predicted labels, associated probability values can also be generated based on the proportion of votes per class.
        
        The last classifier, a support vector machine (SVM) \cite{Vapnik1998}, is also a discriminative classifier. If each row of the feature matrix $X\in\mathbb{R}^{n\times d}$ is treated as a coordinate, then the data can be visualised in the feature-space over $d$ dimensions. SVMs operate to find a hyperplane,
        \begin{talign}
            \vec{w}\cdot\vec{x}-b=0, 
        \end{talign}
        separating the points in the feature-space that best divides the dataset into two classes. The points closest to the hyperplane determine its direction and intercept, these are called the support vectors; the distance between the support vectors and the hyperplane is defined as the margin, bounded by the hyperplanes
        \begin{talign}
            \vec{w}\cdot\vec{x_1}-b &=1&\\
            \vec{w}\cdot\vec{x_2}-b &=-1,&
        \end{talign}
        subject to the constraint that allows no in-sample errors
        \begin{talign}
            y_k(\vec{w_k}\vec{x_k}-b) \geq 1,\text{ for }1\leq k\leq n.
            \label{eq:pl-clf-svmconst}
        \end{talign}
        By maximising the margin, the chance of new data being classified correctly is also maximised. The problem then boils down to a maximisation problem, where the size of the margin is found by considering the distance between points $x_1$ and $x_2$ that lie on a hyperplane perpendicular to the margin,
        \begin{talign}
            \vec{w}\cdot\vec{x_1}-b - 1 - (\vec{w}\cdot\vec{x_2}-b + 1) &=0&\\
            \vec{w}\cdot(\vec{x_1}-\vec{x_2}) &=2&\\
            ||\vec{w}\cdot(\vec{x_1}-\vec{x_2})|| &=2&\\
            ||x_1-x_2|| &= 2/||W||&
        \end{talign}
        where the fact that $\vec{w}$ is parallel to $\vec{x_1}-\vec{x_2}$ has been used to simplify the dot product into the product of magnitudes. The problem is now a case of minimising $||w||$ subject to the constraints given in equation \ref{eq:pl-clf-svmconst}. This is a simple linear example with a `hard margin' which tends to overfit strongly to outliers, reducing generalisability of the model. Instead, a `soft margin' is commonly used, defined by the hinge loss which relaxes constraints to allow for in-sample errors, transforming the problem to
        \begin{talign}
            \argmin_w \frac{1}{n}\sum_{k=1}^n(\max(0,1-y_k(\vec{w}\cdot\vec{x_i}-b)) + \lambda||w||^2)
        \end{talign}
        where the first term is the total hinge loss and the second is regularisation of $\vec{w}$ that controls the width of the margin. If the data is not linearly separable, then the feature-space can be transformed into a higher dimension using any non-linear mapping; popular kernels are the radial basis function kernel and a polynomial function kernel \cite{Scholkopf2002}.
   
        % explain how discriminative not probabilistyic but probaility can still be found
    
        % \begin{sitemize}
        %     \item{big section here, introduce and justify the algorithms used as well as other options that have not been used}
        %     \item{justify no NN because they v data hungry  (Moore et al. 1986; Moore and Miller 2002; Li et al. 2009) [Flying Insect Classification with Inexpensive}
        %     \item{compare unsupervised machine learning and why havent used}
        %     \item{talk about native/none native multiclass handling}
        %     \item{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1397873/ - good svm}
        %     \item{many implemented but only 3 used, reasons in sec XXX}
        % \end{sitemize}
    
    % \subsection{Other Algorithms}
    % \label{subsec:pl-clf-alg}
    %     \begin{sitemize}
    %         \item{summarise shuyus AR detector, not too much detail here}
    %         \item{summarise ivans wavelet detector, not too much detail here}
    %     \end{sitemize}
    \subsection{Software Implementation}
    \label{subsec:pl-clf-software}
        Three classification algorithms, as well as the others tested in section \ref{subsec:exp-clf-select} are all implemented using scikit-learn \cite{Pedregosa2012}. Each classifier is initialised with the given hyperparameters on \code{ClassiSet} initialisation, where methods \code{train} and \code{apply} can subsequently be called. In the case of the multi-class label aggregation policy, discussed in section \ref{subsec:pl-data-software}, there are slight differences to the binary classification pipeline. First, the classifier must, obviously, support multi-class classification. The SVM implementation does not (natively) and instead a problem transformation is performed. Given that there are $K$ classes, the transform `One-Vs.-One' trains $K(K-1)/2$ binary classifiers where each receives samples of two classes from he original training set. The `One-Vs.-Rest' transform trains $K$ classifiers using all the samples from class $k$ as positive examples and the remaining samples as negative. Both of these transformed problems then apply a voting scheme at classification time; all classifiers are run on an unseen sample and the majority vote is taken. As well as classification issues, there are also difficulties with the format of the output. Should the number of label sets be even, then there is a possibility that there will cases in which agreement is split down the middle. This corresponds to predicting sample belonging to class `$0.5$' and can be handled in three ways: accept as a mosquito, accept as no mosquito or reject the prediction completely. Probability values are obtained by summing the probabilities over the relevant classes.
        % \begin{sitemize}
        %     \item{all done with sklearn, multiclass handling too with native/ovr/ovo, every result passed to postproc to make same binary format}
        %     \item{default params}
        %     \item{modularity}
        %     \item{how results stored}
        % \end{sitemize}